# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xOomtESCRi5vQlVWNUtfuU5CjA2SOoMT
"""



import pandas as pd
import streamlit as st
from langchain.agents import AgentType
from langchain_experimental.agents import create_pandas_dataframe_agent
from langchain_ollama import ChatOllama
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


st.set_page_config(
    page_title="Chat with sheet",
    page_icon="ðŸ“Š",
    layout="centered"
)

def read_data(file):
  if file.name.endswith(".csv"):
    return pd.read_csv(file)
  elif file.name.endswith(".xlsx"):
    return pd.read_excel(file)

st.title("Chat with your sheet")

if "chat_history" not in st.session_state:
  st.session_state.chat_history = []

#initializing dataframe
if "df" not in st.session_state:
  st.session_state.df = None

uploaded_file = st.file_uploader("Upload a file", type=["csv", "xlsx", "xls"])

if uploaded_file:
  st.session_state.df = read_data(uploaded_file)
  st.write("Dataframe preview")
  st.write(st.session_state.df.head())

#show previous history of chat
for message in st.session_state.chat_history:
  with st.chat_message(message["role"]):
    st.markdown(message["content"])

user_prompt = st.chat_input("Ask a question")

#add user message to chat history
if user_prompt:
  st.chat_message("user").markdown(user_prompt)
  st.session_state.chat_history.append({"role": "user", "content": user_prompt})

  llm = ChatOllama(model="gemma-2b", temperature=0)

  pandas_df_agent = create_pandas_dataframe_agent(
    llm,
    st.session_state.df,
    agent_type = AgentType.OPENAI_FUNCTIONS,
    verbose=True,
    allow_dangerous_code = True
      )

  messages = [
    {role: "system", content: "You are a helpful assistant."},
    *st.session_state.chat_history
     ]

  response = pandas_df_agent.invoke(user_prompt)

  assistant_response = response["output"]

  st.session_state.chat_history.append({"role": "assistant", "content": assistant_response})

  with st.chat_message("assistant"):
    st.markdown(assistant_response)
